{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2670dad4-ace3-4902-98d8-ff77a3dc2676",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor regParam: 0.001 | F1 (train-val): 0.815\n[TEST] Accuracy: 0.894 | F1: 0.890 | wPrec: 0.894 | wRec: 0.894\nTop 15 variables por importancia (suma |coef| OVR):\nCrossing: 0.0226\nStamina: 0.0172\nHeading_Accuracy: 0.0170\nReactions: 0.0165\nSliding_Tackle: 0.0156\nDribbling: 0.0118\nLong_Passing: 0.0117\nVision: 0.0113\nMarking: 0.0105\nFinishing: 0.0104\nSprint_Speed: 0.0101\nGK_Handling: 0.0085\nAgility: 0.0082\nGK_Diving: 0.0080\nGK_Reflexes: 0.0072\n+--------------------+-------------------+------------------+------------------+----------+\n|score_GK            |score_DEF          |score_MID         |score_OFF         |prediction|\n+--------------------+-------------------+------------------+------------------+----------+\n|-0.02538900248718938|-0.1252865572817458|0.3996523369762138|0.7201159743118317|3.0       |\n+--------------------+-------------------+------------------+------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# --- imports\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# =========================\n",
    "# 0) Datos base (df_top)\n",
    "# =========================\n",
    "df = spark.table(\"fifa_21_male_2\")\n",
    "\n",
    "top_clubs = [\n",
    "    \"Manchester City\",\"Liverpool\",\"Chelsea\",\"Arsenal\",\"Manchester United\",\"Tottenham Hotspur\",\n",
    "    \"Real Madrid\",\"FC Barcelona\",\"Atlético Madrid\",\"Sevilla FC\",\"Villarreal CF\",\n",
    "    \"Juventus\",\"Inter\",\"AC Milan\",\"Napoli\",\"AS Roma\",\n",
    "    \"Bayern Munich\",\"Borussia Dortmund\",\"RB Leipzig\",\"Bayer 04 Leverkusen\",\n",
    "    \"Paris Saint-Germain\",\"Olympique Lyonnais\",\"AS Monaco\",\"Marseille\"\n",
    "]\n",
    "df_top = df.filter(col(\"Club\").isin(top_clubs))\n",
    "\n",
    "# =========================\n",
    "# 1) Seleccionar SOLO columnas del rename_map y renombrar\n",
    "# =========================\n",
    "rename_map = {\n",
    "    \"Crossing\": \"Crossing\",\n",
    "    \"Finishing\": \"Finishing\",\n",
    "    \"Heading Accuracy\": \"Heading_Accuracy\",\n",
    "    \"Short Passing\": \"Short_Passing\",\n",
    "    \"Volleys\": \"Volleys\",\n",
    "    \"Dribbling\": \"Dribbling\",\n",
    "    \"Curve\": \"Curve\",\n",
    "    \"FK Accuracy\": \"FK_Accuracy\",\n",
    "    \"Long Passing\": \"Long_Passing\",\n",
    "    \"Ball Control\": \"Ball_Control\",\n",
    "    \"Acceleration\": \"Acceleration\",\n",
    "    \"Sprint Speed\": \"Sprint_Speed\",\n",
    "    \"Agility\": \"Agility\",\n",
    "    \"Reactions\": \"Reactions\",\n",
    "    \"Balance\": \"Balance\",\n",
    "    \"Shot Power\": \"Shot_Power\",\n",
    "    \"Jumping\": \"Jumping\",\n",
    "    \"Stamina\": \"Stamina\",\n",
    "    \"Strength\": \"Strength\",\n",
    "    \"Long Shots\": \"Long_Shots\",\n",
    "    \"Aggression\": \"Aggression\",\n",
    "    \"Interceptions\": \"Interceptions\",\n",
    "    \"Positioning\": \"Positioning\",\n",
    "    \"Vision\": \"Vision\",\n",
    "    \"Penalties\": \"Penalties\",\n",
    "    \"Composure\": \"Composure\",\n",
    "    \"Marking\": \"Marking\",\n",
    "    \"Standing Tackle\": \"Standing_Tackle\",\n",
    "    \"Sliding Tackle\": \"Sliding_Tackle\",\n",
    "    \"GK Diving\": \"GK_Diving\",\n",
    "    \"GK Handling\": \"GK_Handling\",\n",
    "    \"GK Kicking\": \"GK_Kicking\",\n",
    "    \"GK Positioning\": \"GK_Positioning\",\n",
    "    \"GK Reflexes\": \"GK_Reflexes\",\n",
    "    \"BP\": \"BP\"   # etiqueta POS detallada\n",
    "}\n",
    "\n",
    "# Tomar solo las columnas del mapping que realmente existen en df_top\n",
    "present_cols = [c for c in rename_map.keys() if c in df_top.columns]\n",
    "if \"BP\" not in present_cols:\n",
    "    raise ValueError(\"La columna 'BP' no está en el dataset. Revisa el nombre de la columna objetivo.\")\n",
    "\n",
    "df_w = df_top.select(*present_cols)\n",
    "for old, new in rename_map.items():\n",
    "    if old in df_w.columns and old != new:\n",
    "        df_w = df_w.withColumnRenamed(old, new)\n",
    "\n",
    "# =========================\n",
    "# 2) Mapear BP (posiciones FIFA finas) → RoleGroup {GK, DEF, MID, OFF}\n",
    "# =========================\n",
    "def_positions = [\"CB\",\"LB\",\"RB\",\"LWB\",\"RWB\"]\n",
    "mid_positions = [\"CDM\",\"CM\",\"LM\",\"RM\"]\n",
    "off_positions = [\"ST\",\"CF\",\"CAM\",\"LW\",\"RW\",\"LAM\",\"RAM\",\"LF\",\"RF\"]\n",
    "\n",
    "df_w = df_w.withColumn(\n",
    "    \"RoleGroup\",\n",
    "    F.when(F.col(\"BP\")==\"GK\", \"GK\")\n",
    "     .when(F.col(\"BP\").isin(def_positions), \"DEF\")\n",
    "     .when(F.col(\"BP\").isin(mid_positions), \"MID\")\n",
    "     .when(F.col(\"BP\").isin(off_positions), \"OFF\")\n",
    "     .otherwise(F.lit(None))\n",
    ")\n",
    "\n",
    "# Filtrar filas válidas (descarta posiciones raras o NaN)\n",
    "df_w = df_w.filter(F.col(\"RoleGroup\").isNotNull())\n",
    "\n",
    "# =========================\n",
    "# 3) Features vector + label numérica\n",
    "# =========================\n",
    "# Features = TODAS las columnas del mapping excepto BP (ya renombradas)\n",
    "feature_cols = [new for (old,new) in rename_map.items() if new in df_w.columns and new != \"BP\"]\n",
    "\n",
    "# UDF para vectorizar (podrías usar VectorAssembler; acá dejo tu estilo original)\n",
    "to_vector = F.udf(lambda xs: Vectors.dense([float(x) if x is not None else 0.0 for x in xs]), VectorUDT())\n",
    "df_vec = df_w.withColumn(\n",
    "    \"features\",\n",
    "    to_vector(F.array(*[F.col(c).cast(\"double\") for c in feature_cols]))\n",
    ")\n",
    "\n",
    "# Label mapping en el orden: GK=0, DEF=1, MID=2, OFF=3\n",
    "label_map = F.when(F.col(\"RoleGroup\")==\"GK\", F.lit(0.0)) \\\n",
    "             .when(F.col(\"RoleGroup\")==\"DEF\", F.lit(1.0)) \\\n",
    "             .when(F.col(\"RoleGroup\")==\"MID\", F.lit(2.0)) \\\n",
    "             .when(F.col(\"RoleGroup\")==\"OFF\", F.lit(3.0))\n",
    "df_vec = df_vec.withColumn(\"label\", label_map)\n",
    "\n",
    "# Split\n",
    "train, test = df_vec.select(\"features\",\"label\").randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# =========================\n",
    "# 4) OVR con Lasso (LinearRegression L1)\n",
    "# =========================\n",
    "class_names = [\"GK\",\"DEF\",\"MID\",\"OFF\"]   # debe corresponder al label mapping 0..3\n",
    "\n",
    "def fit_ovr_lasso(df_train, reg_param):\n",
    "    models = {}\n",
    "    for idx, name in enumerate(class_names):\n",
    "        y_col = F.when(F.col(\"label\")==float(idx), F.lit(1.0)).otherwise(F.lit(0.0)).alias(\"y\")\n",
    "        train_y = df_train.select(\"features\", y_col)\n",
    "        lr = LinearRegression(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=\"y\",\n",
    "            elasticNetParam=1.0,     # L1 puro\n",
    "            regParam=reg_param,\n",
    "            maxIter=200,\n",
    "            standardization=True\n",
    "        )\n",
    "        models[name] = lr.fit(train_y)\n",
    "    return models\n",
    "\n",
    "def add_scores(df_in, models):\n",
    "    out = df_in\n",
    "    for name, m in models.items():\n",
    "        out = m.transform(out).withColumnRenamed(\"prediction\", f\"score_{name}\")\n",
    "        # limpieza de columnas temporales\n",
    "        for col_drop in [\"y\", \"prediction\"]:\n",
    "            if col_drop in out.columns:\n",
    "                out = out.drop(col_drop)\n",
    "    return out\n",
    "\n",
    "def predict_class(df_in):\n",
    "    score_cols = [f\"score_{n}\" for n in class_names]\n",
    "    pred_idx = F.array_max(F.array(*[F.col(c) for c in score_cols]))\n",
    "    return (\n",
    "        df_in\n",
    "        .withColumn(\"pred_idx\", pred_idx)\n",
    "        .withColumn(\n",
    "            \"prediction\",\n",
    "            F.when(F.col(\"pred_idx\")==F.col(\"score_GK\"),  F.lit(0.0))\n",
    "             .when(F.col(\"pred_idx\")==F.col(\"score_DEF\"), F.lit(1.0))\n",
    "             .when(F.col(\"pred_idx\")==F.col(\"score_MID\"), F.lit(2.0))\n",
    "             .otherwise(F.lit(3.0)) # OFF\n",
    "        )\n",
    "        .drop(\"pred_idx\")\n",
    "    )\n",
    "\n",
    "# =========================\n",
    "# 5) Búsqueda simple de regParam\n",
    "# =========================\n",
    "reg_grid = [1e-4, 1e-3, 1e-2, 1e-1, 3e-1]\n",
    "best_r, best_f1 = None, -1.0\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "for r in reg_grid:\n",
    "    models_r = fit_ovr_lasso(train, r)\n",
    "    val_scores = add_scores(train, models_r)  # si querés, separá un val split\n",
    "    val_pred = predict_class(val_scores)\n",
    "    f1 = evaluator.evaluate(val_pred)\n",
    "    if f1 > best_f1:\n",
    "        best_f1, best_r = f1, r\n",
    "\n",
    "print(f\"Mejor regParam: {best_r} | F1 (train-val): {best_f1:.3f}\")\n",
    "\n",
    "# =========================\n",
    "# 6) Entrenar con mejor regParam y evaluar en test\n",
    "# =========================\n",
    "best_models = fit_ovr_lasso(train, best_r)\n",
    "\n",
    "test_scored = add_scores(test, best_models)\n",
    "test_pred   = predict_class(test_scored)\n",
    "\n",
    "acc = MulticlassClassificationEvaluator(metricName=\"accuracy\", labelCol=\"label\").evaluate(test_pred)\n",
    "f1  = MulticlassClassificationEvaluator(metricName=\"f1\", labelCol=\"label\").evaluate(test_pred)\n",
    "wp  = MulticlassClassificationEvaluator(metricName=\"weightedPrecision\", labelCol=\"label\").evaluate(test_pred)\n",
    "wr  = MulticlassClassificationEvaluator(metricName=\"weightedRecall\", labelCol=\"label\").evaluate(test_pred)\n",
    "\n",
    "print(f\"[TEST] Accuracy: {acc:.3f} | F1: {f1:.3f} | wPrec: {wp:.3f} | wRec: {wr:.3f}\")\n",
    "\n",
    "# =========================\n",
    "# 7) Importancia (suma de |coef| en las 4 clases)\n",
    "# =========================\n",
    "import numpy as np\n",
    "coef_sum_abs = np.zeros(len(feature_cols), dtype=float)\n",
    "for name in class_names:\n",
    "    coef_sum_abs += np.abs(best_models[name].coefficients.toArray())\n",
    "\n",
    "ranking = sorted([(f, float(w)) for f, w in zip(feature_cols, coef_sum_abs)],\n",
    "                 key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top 15 variables por importancia (suma |coef| OVR):\")\n",
    "for feat, w in ranking[:15]:\n",
    "    print(f\"{feat}: {w:.4f}\")\n",
    "\n",
    "# =========================\n",
    "# 8) Predicción de ejemplo (usando una fila real del dataset para asegurar dimensiones)\n",
    "# =========================\n",
    "example = df_w.limit(1)\n",
    "example_vec = example.select(\n",
    "    to_vector(F.array(*[F.col(c).cast(\"double\") for c in feature_cols])).alias(\"features\")\n",
    ")\n",
    "\n",
    "ex_scored = add_scores(example_vec, best_models)\n",
    "ex_pred = predict_class(ex_scored)\n",
    "ex_pred.select(\"score_GK\",\"score_DEF\",\"score_MID\",\"score_OFF\",\"prediction\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3367d06d-ce5a-4ce1-bbb0-980acbb16ce4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n=== MATRIZ DE CONFUSIÓN ===\n\n            GK (pred)  DEF (pred)  MID (pred)  OFF (pred)\nGK (real)        17.0         0.0         0.0         0.0\nDEF (real)        0.0        37.0         0.0         0.0\nMID (real)        0.0         4.0        20.0         5.0\nOFF (real)        0.0         1.0         3.0        36.0\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.sql import functions as F\n",
    "\n",
    "# Contar cada combinación real vs predicho\n",
    "cm = (test_pred\n",
    "      .groupBy(\"label\",\"prediction\")\n",
    "      .count()\n",
    "      .orderBy(\"label\",\"prediction\"))\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Pasar a pandas y pivotear\n",
    "cm_pdf = cm.toPandas().pivot(index=\"label\", columns=\"prediction\", values=\"count\").fillna(0)\n",
    "\n",
    "# Reordenar columnas por si falta alguna\n",
    "cm_pdf = cm_pdf.reindex(index=[0,1,2,3], columns=[0,1,2,3], fill_value=0)\n",
    "\n",
    "# Renombrar índices a nombres legibles\n",
    "cm_pdf.index = [\"GK (real)\", \"DEF (real)\", \"MID (real)\", \"OFF (real)\"]\n",
    "cm_pdf.columns = [\"GK (pred)\", \"DEF (pred)\", \"MID (pred)\", \"OFF (pred)\"]\n",
    "\n",
    "print(\"\\n=== MATRIZ DE CONFUSIÓN ===\\n\")\n",
    "print(cm_pdf.to_string())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-10-27 19:19:26",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}